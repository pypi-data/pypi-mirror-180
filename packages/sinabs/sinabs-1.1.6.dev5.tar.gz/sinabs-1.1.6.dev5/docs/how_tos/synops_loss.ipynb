{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimise the number of synaptic operations\n",
    "As described in [Sorbaro et al. 2020](https://www.frontiersin.org/articles/10.3389/fnins.2020.00662/full), it can be beneficial for power consumption and latency in the network to reduce the number of synaptic operations (synops). This number is essentially the output activation multiplied by the number of outward connections (fan-out) to the next layer. We describe how using Sinabs' synops counters it's possible to easily add a term to your loss function which then can be minimised.\n",
    "\n",
    "## Training an ANN with fewer synops\n",
    "Let's start by defining our ANN. Keep in mind that we use NeuromorphicRelus here as we need to discretize the output in the forward pass to simulate the number of spikes that layer would emit. In the backward pass the derivative of the ReLU function is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sinabs\n",
    "import sinabs.layers as sl\n",
    "\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 5, bias=False),\n",
    "    sl.NeuromorphicReLU(),\n",
    "    sl.SumPool2d(2),\n",
    "    nn.Conv2d(16, 32, 5, bias=False),\n",
    "    sl.NeuromorphicReLU(),\n",
    "    sl.SumPool2d(2),\n",
    "    nn.Conv2d(32, 120, 4, bias=False),\n",
    "    sl.NeuromorphicReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, 10, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a SynopsCounter to our ANN to track how many synaptic operations it would need in an SNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "synops_counter_ann = sinabs.SynOpCounter(ann.modules(), sum_activations=True)\n",
    "\n",
    "rand_input = torch.rand((batch_size, 1, 28, 28))\n",
    "ann(rand_input)\n",
    "print(f\"Synops after feeding input: {synops_counter_ann()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an SNN with fewer synops\n",
    "Let's start by defining your SNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Sequential):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__(\n",
    "            sl.FlattenTime(),\n",
    "            nn.Conv2d(1, 16, 5, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.SumPool2d(2),\n",
    "            nn.Conv2d(16, 32, 5, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.SumPool2d(2),\n",
    "            nn.Conv2d(32, 120, 4, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(120, 10, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.UnflattenTime(batch_size=batch_size),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def spiking_layers(self):\n",
    "        return [layer for layer in self.net.children() if isinstance(layer, sl.StatefulLayer)]\n",
    "\n",
    "    def reset_states(self):\n",
    "        for layer in self.spiking_layers:\n",
    "            layer.reset_states()\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "snn = SNN(batch_size=batch_size)\n",
    "snn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply a SynopsLossCounter to the model, we'll be able to track the number of synops as we feed new inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synops_counter = sinabs.SNNSynOpCounter(snn)\n",
    "print(f\"Synops before feeding input: {synops_counter.get_total_synops()}\")\n",
    "\n",
    "rand_input_spikes = (torch.rand((batch_size, 10, 1, 28, 28)) < 0.1).float()\n",
    "y_hat = snn(rand_input_spikes)\n",
    "print(f\"Synops after feeding input: {synops_counter.get_total_synops()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get a more detailed count for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synops_counter.get_synops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have can calculate the total synops, we might want to choose a target synops number in order to decrease power consumption. As a rule of thumb we're going to take half of the number of initial synops as constant target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done once\n",
    "target_synops = synops_counter.get_total_synops() / 2\n",
    "\n",
    "# in your training loop\n",
    "synops = synops_counter.get_total_synops()\n",
    "synops_loss = (target_synops - synops).square() / target_synops.square()\n",
    "\n",
    "loss = y_hat.sum(1) + synops_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "65b6f4b806bbaf5b54d6ccaa27abf7e5307b1f0e4411e9da36d5256169cebdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
