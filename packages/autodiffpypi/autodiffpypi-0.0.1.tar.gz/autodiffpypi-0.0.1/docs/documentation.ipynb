{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode AD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Automatic Differentiation allows for fast and precise derivitative computation. It is not, however, the only method for computing derivatives. \n",
    "\n",
    "Derivatives are a fundamental tool in mathematics with applications in a range of scientific fields.  Differentiation is core to many computational problems including finding extrema and zeroes of functions. Optimization problems such as gradient descent in neural networks depend on partial derivatives to incrementally move towards optimal solutions. Directional derivatives and the Jacobian matrix are utilized in a variety of contexts including backpropogation in training neural networks. Directional derivatives compute the change in a mult-dimensional function with a respect to any linear combination of its input parameters. \n",
    "\n",
    "Because of the prevalence of derivatives in computational fields, automatic differentiation focuses on developing algorithmic techniques for efficiently evaluating derivatives.\n",
    "\n",
    "\n",
    "### Symbolic differentiation\n",
    "Symbolic differentiation applies basic derivative rules to compute derivatives at the expression level. While precise, symbolic differentiation uses excess computational power and memory to hold the swelling equation resulting from chain rule. Symbolic differentiation computes the derivative of a function analytically, which offers high precision at the cost of execution time and exponential size in relation to the original function. Since many science applications care about the derivative evaluated at a value, we can often bypass the need for an analytic form for the derivative.\n",
    "\n",
    "### Finite difference method\n",
    "The finite difference method, on the other hand, estimates the derivative by computing the slope of the secant line, using a small change in $x$. The precision of the derivative depends on the choice of $x$; too large of a step leads to precision error but too small of a step creates rounding error.  Numerical differentiation, on the other hand, offers O(n) time computation for an n-dimensional gradient (think multi-dimensional derivative). It achieves this by estimating the derivative instead of computing it precisely. The trade-off, as with all estimations, is a reduction in accuracy due to rounding and truncation.\n",
    "\n",
    "### Why AutoDiff?\n",
    "Automatic Differentiation achieves the precision of symbolic differentiation with time complexity that matches the complexity of computing the original function. It does this by leveraging a useful fact: when computing a derivative at a specific point, we don't need the symbolic expression of the derivative. This allows us to compute our derivative via a series of computations that compute each depending on the prior computation rather than deriving the entire expression and plugging in the point at the end.\n",
    "\n",
    "In forward mode, the graph can be abstract as computations are made on-the-fly and no storage of all prior computations is needed. In reverse-mode, on the other hand, space needs will exceed those of computing the original function as the computational graphs of derivatives will need to be stored and computed in reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Atomatic differentiation (AD) is based on the property of decomposition of a function into intermediate steps based on the chain rule: $$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "Consider the function $f(x_1, x_2) = e^{-(\\sin(x_1) + \\cos(x_2)}$. \n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1q8eWiv6gv-ty2rv-S0OuEOqwmeVhsLgq)\n",
    "\n",
    "The derivative is derived by chaining together the product of the outer derivative with each of the inner derivatives. This creates a computational graph, where each edge represents an elementary operation applied to an intermediary node.\n",
    "\n",
    "Forward mode AD uses forward accumulation of these intermediary steps, traversing the chain rule in a partial ordering from the most inner part (the original inputs) working outward until the entire derivative has been evaluated. In addition, at each step we compute the derivative of the intermediate variables, called the tangent trace. This allows for us to compute the directional derivative in a single pass.\n",
    "\n",
    "Dual numbers are convenient for differentiation since can allow us to capture both the primal and tangent trace at the same time. A dual number can be expressed as $a + b\\epsilon$, where $a$ and $b$ are real numbers and $\\epsilon$ is small enough such that $\\epsilon^2 = 0$. The real part stores the primal trace while the dual part stores the tangent trace.\n",
    "\n",
    "Forward mode automatic differentiation takes a seed vector which determines the direction of the derivative. This can be any linear combination of derivatives. To compute the full Jacobian using Forward Mode AD, this requires performing $m$ passes where $m$ is the number of inputs to the original function. On each pass, we calculate the derivative with respect to that direction.\n",
    "\n",
    "In contrast, Reverse mode AD enables us to compute the full gradient in two passes. This requires storing an explicit computational graph. In the forward pass, reverse mode computes the primal trace and partial derivatives with respect to the parent node. In the reverse pass, these are reconstructed via the chain rule by accumulating the values of all child nodes.\n",
    "\n",
    "Runtime of Forward and Reverse modes -\n",
    "\n",
    "When a function is defined as $ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $, Forward Mode computes the gradient with respect to the number of independent variables.\n",
    "Therefore, the computational cost of using Forward Mode depends on the number of independent variables n.\n",
    "\n",
    "Reverse Mode computes the gradient with respect to the number of outputs. Therefore, the computational cost of using Forward Mode depends on the number of outputs, m.\n",
    "\n",
    "As a result, if there are more outputs than inputs, Forward mode is more efficient.\n",
    "On the other hand, if there are more inputs than outputs, Reverse mode is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `AutoDiffPy`\n",
    "\n",
    "#### Installation\n",
    "The following are the steps for installing the AutoDiffPy package and getting started. Copy the SSH url for the Github repo, then follow the below steps in your terminal.\n",
    "1. `git clone <repo>`\n",
    "2. `cd <repo>`\n",
    "3. `pip install virtualenv` (if you don't already have it installed)\n",
    "4. `python -m venv autodiff_venv`\n",
    "5. `source autodiff_venv/bin/activate`\n",
    "6. `pip install -r requirements.txt`\n",
    "\n",
    "#### Running the tests\n",
    "Run the command `pytest` to run all test suites. Provide an optional file parameter, e.g. `pytest tests/test_dual_number.py` to run the specified test.\n",
    "\n",
    "To view a coverage report, run the following command `python3 -m pytest --cov=src/AutoDiffPy --cov-fail-under=90`.\n",
    "\n",
    "#### Step-By-Step Usage\n",
    "After completing the installation instructions above, follow the below steps to begin working with the `AutoDiffPy` package:\n",
    "\n",
    "1. Import the package and `numpy` at the top of your Python file.\n",
    "\n",
    "```\n",
    "import src.AutoDiffPy as adp\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "To import all functions from the package, use the following syntax, `from src.AutoDiffPy import *`, which allows you to use all functions without prepending the package name.\n",
    "\n",
    "```\n",
    ">>> from src.AutoDiffPy import *\n",
    ">>> x = 2\n",
    ">>> f = lambda x: sin(x)\n",
    ">>> ...\n",
    "```\n",
    "\n",
    "2. Initialize variables to their desired values.\n",
    "\n",
    "```\n",
    "x = 2\n",
    "y = np.sin(0.5)\n",
    "z = 12352.124\n",
    "```\n",
    "\n",
    "3. Define functions using the `lambda` syntax and any relevant variables.\n",
    "\n",
    "```\n",
    "f1 = lambda x : x - 5\n",
    "f2 = lambda y, z : 3 * y ** 2 - log(x)\n",
    "```\n",
    "\n",
    "4. Create a Forward Mode object and pass in a function or non-empty array of functions. The second parameter should be the number of inputs to the provided functions (dimension).\n",
    "\n",
    "```\n",
    "fwd = Forward(f1, 1)\n",
    "fwd1 = Forward([f1, f2], 3)\n",
    "```\n",
    "\n",
    "5. Evaluate the functions or set of functions using the `evaluate()` method, providing the relevant variables as parameters.\n",
    "```\n",
    "fwd.evaluate(x)\n",
    "fwd1.evaluate(x, y, z)\n",
    "```\n",
    "\n",
    "6. Compute the derivative of the function(s) using the `derivative(index, *args)` method, providing the relevant variables as parameters. For instance, `derivative(x)` will evaluate the partial derivative with respect to `x`, while `derivative(y)` will evaluate the derivative with respect to `y`. If no second parameter is provided, the function will default to differentiating with respect to the first variable of the function.\n",
    "\n",
    "```\n",
    "fwd.derivative(x)\n",
    "fwd1.derivative(y)\n",
    "```\n",
    "\n",
    "7. Compute the jacobian of the function(s) using the `jacobian(*args)` method. The `args` parameter specifies the variables whose partial derivatives will be populated into the Jacobian matrix.\n",
    "\n",
    "```\n",
    "fwd.jacobian(x)\n",
    "fwd1.jacobian(x, y, z)\n",
    "```\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "#### dual_number.py\n",
    "\n",
    "The file `dual_number.py` implements the DualNumber class with support for scalar functions. For the final milestone this will expand to arbitrary dimensions.\n",
    "\n",
    "The class has no internal dependencies and the only external dependency is Numpy.\n",
    "\n",
    "The DualNumber class implements the following functions:\n",
    "* addition and subtraction (__add__, __sub__, __radd__, __rsub__)\n",
    "* multiplication and division (__mul__, __truediv__, __rmul__, __rtruediv__)\n",
    "* negation (__neg__)\n",
    "* comparison operators (__eq__, __leq__, __gt__, __le__, etc.)\n",
    "* exponentiation (__pow__, __rpow__)\n",
    "\n",
    "#### base_derivatives.py\n",
    "\n",
    "The file `base_derivatives.py` implements the base derivatives for basic functions. The class depends on the `DualNumber` class and Numpy.\n",
    "\n",
    "Each function has support for dual number, integer, or float.\n",
    "\n",
    "The base_derivatives implements the following functions:\n",
    "* Trig functions - sin, cos, tan\n",
    "* Inverse trig functions - arcsin, arccos, arctan\n",
    "* Hyperbolic functions - sinh, cosh, tanh\n",
    "* The exponentials function, square root function, logarithmic function, power function, and sigmoid function.\n",
    "\n",
    "These functions allow us to directly compute derivatives at each stage of the pass and store them, the evaluation of the function in the real part of the dual number, and the derivative of the function in the dual part of the dual number (in the case that we are not given a DualNumber, only an integer or float, the function is being evaluated and returned - the derivative is being discarded).\n",
    "\n",
    "#### forward.py\n",
    "\n",
    "The file `forward_mode.py` implements the Forward mode class  with support for single-dimensional (scalar valued) functions.\n",
    "\n",
    "The class has a dependency on the DualNumber class (see `dual_number.py`), as the primal and tangent trace are collected in the real and dual parts of the Dual number object, respectively. The dual number class contains function overrides for elementary operations on Dual numbers, including:\n",
    "* addition and subtraction (__add__, __sub__)\n",
    "* multiplication and division (__mul__, __truediv__)\n",
    "* negation (__neg__)\n",
    "* comparison operators (__eq__, __leq__, __gt__, __le__, etc.)\n",
    "* exponentiation (__pow__)\n",
    "\n",
    "The forward mode of this AD library is supplied with an input function by the user, which may consist of any number of nested intermediate function applications. For example:\n",
    "* f(a) = a\n",
    "* f(b) = sin(b)\n",
    "* f(c) = 1 / (tanh(c) + c)**c\n",
    "* f(d) = log(c, 10) + c**2\n",
    "\n",
    "These intermediate functions, such as `sin`, `tanh`, `log`, and exponentiation, must be both evaluated and differentiated during the AD process. The file `base_derivatives.py` computes the derivatives of these elementary functions with support for type DualNumber as well as floats and integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization \n",
    "\n",
    "Our initial software organization plan is to store our AutoDiff classes in a source folder, our Test Suite in a test folder, an example script in an example folder, and a setup script for installing relevant packages at the root level. This provides a clear distinction between core functionality, testing, examples, and setup. \n",
    "\n",
    "```\n",
    "team12/\n",
    "├── /docs\n",
    "│   └── milestone1\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "└── /src\n",
    "│   └── /core\n",
    "│   │   ├── base_derivatives.py (holds common basic derivatives e.g. trig)\n",
    "│   │   ├── dual_number.py (holds override Dunder methods)\n",
    "│   │   └── override_funcs.py (holds basic math functions for dual numbers)\n",
    "│   ├── forward.py (holds forward pass class)\n",
    "│   ├── reverse.py (holds reverse pass class)\n",
    "│   └── /structures\n",
    "│       └── graph.py (Reverse: node class)\n",
    "├── /tests\n",
    "│     ├── individual test files per module\n",
    "│     └── test.py (runs all tests and delivers relevant output)\n",
    "├── /examples\n",
    "│     └── app.py\n",
    "└── setup.py (runs relevant instillations)\n",
    "\n",
    "```\n",
    "At the root level of our source folder we have the base classes that form our AutoDiff implementation. Core will hold a class that includes functions that compute primitive derivatives and a dual number class with its dunder methods overriden to work as dual numbers. \n",
    "\n",
    "In adition to core, the source folder holds two AD method classes, forward and reverse. For reverse, we need to store the computational graph and nodes and therefore these are stored in a structures folder at the same level as reverse.\n",
    "\n",
    "### Distribution\n",
    "\n",
    "For the final release, we will distribute AutoDiffPy via PyPI with PEP517/518. The current state uses a simple `requirements.txt` file to manage dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation Details\n",
    "\n",
    "This section provides a more rigorous explanation of the technical design and implementation.\n",
    "\n",
    "### Classes\n",
    "\n",
    "#### Base Derivatives\n",
    "\n",
    "* Motivation: the Base Derivatives class will hold direct computations of primitive derivatives such as logarithmic, exponential, trig, and logistic functions. This will allow us to directly compute derivatives at each stage of the pass. \n",
    "* Arguments: each method will take in a function $f$ and output $f'$\n",
    "* Methods: The methods will include log, exp, trig functions and there inverses, logistic.\n",
    "\n",
    "#### Dual Number\n",
    "\n",
    "* Motivation: the Dual Number class will construct the Dual Number objects that will be used in our Forward-Mode.\n",
    "* Arguments: The `__init__` method will take in an int / float as a first argument for the real part of the Dual Number. The second argument, Dual, will be optional and default to $1$. \n",
    "* Methods: This class will have methods that override the Dunder methods of `__add__`, `__pow__`, `__mul__`, `__radd__`, `__rmul__`, `__truediv__`, `__repr__`. This operator overriding will ensure that basic mathematical operations work on Dual Numbers as well as combining them with ints and floats. Critical for the Dunder methods is that they return new Dual Numbers such that they don't return one of the inputs.\n",
    "\n",
    "#### Override Functions\n",
    "\n",
    "* Motivation: we need to be able to perform basic mathematical computations such as trig functions, exponentials, logarithmic, and logistic using Dual Numbers.\n",
    "* Arguments: Each function will take as an argument the expression (can be a Dual Number or real number) and output the value after passing the argument through the function.\n",
    "* Methods: This file will include independent methods such as: `sin(arg)`, `cos(arg)`, `exp(arg)`, `log_base(arg,base)`. These functions will operate on both Dual Numbers, capturing the Dual component as needed, and real numbers.\n",
    "\n",
    "#### Forward\n",
    "\n",
    "* Motivation: the Forward-Mode class runs Forward-Mode AD on a function $f$.\n",
    "* Arguments: the class takes a function `f` in its `__init__` method.\n",
    "* Methods:\n",
    "  * compute_derivative accepts an input variable and evaluates the derivative of $f$ at the variable.\n",
    "  * compute_jacobian computes the Jacobian via computing the derivative on all the input variables.   \n",
    "\n",
    "\n",
    "#### Reverse \n",
    "\n",
    "* Motivation: the Reverse-Mode class runs Reverse-Mode AD on a function $f$. \n",
    "* Arguments: the Reverse-Mode class when initialized will accept a function $f$ and a computational graph comprising Nodes.\n",
    "* Methods: the Reverse-Mode class will have $sort$, get_derivative, get_jacobian, and backward_pass methods. More on these methods in M2: extension.\n",
    "\n",
    "#### Node\n",
    "\n",
    "* Motivation: the Node module will hold the Node class that forms the computational graph needed for Reverse-Mode AD. \n",
    "* Node:\n",
    "  * Arguments: the Node will be initialized with two arguments: a value and a local gradient. It will also hold an array of child nodes to form the graph.\n",
    "  * Methods: the Node class' Dunder methods will be overriden such that they return new Nodes.\n",
    "\n",
    "### Data Structures\n",
    "\n",
    "The data structures used will be the computational graph via the Nodes stored in the Graph class. \n",
    "\n",
    "### External Libraries\n",
    "* Numpy will be used for computation using vectors and matrices.\n",
    "* Pytest will be used for testing our classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "The feedback received from Milestone 1 was positive overall and our design does not require significant updates. In this revision, we included an additional section in `Implementation` to include the override functions for sin, cos, pow, exp, etc. as suggested in the feedback. The `Software Organization` directory was also updated to reflect this additional file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features\n",
    "\n",
    "#### Reverse Mode\n",
    "Now that you've got most of the hard implementation work done, what kinds of things do you want to implement next?\n",
    "- We plan to implement Reverse Mode AD next\n",
    "\n",
    "How will your software change?\n",
    "- Our software organization will not change because we already had accounted for reverse mode in our milestone 1 documentation. It has a `reverse.py` file in the src directory, and a `graph.py` file in the structures directory which will be the home of the node class used in reverse AD. These are currently outlined in an open pull request and will be fully implemented and tested prior to the final milestone.\n",
    "\n",
    "What will be the primary challenges to implementing these new features?\n",
    "- Reverse mode will introduce greater data structure requirements than forward mode. Dual numbers are not very useful in reverse mode since we don't apply the chain rule directly -- reverse mode computes the sensitivity of f with respect to the independent *and* intermediate variables. This means creating a new data structure we can traverse, and use basic operations on, hence the Node class.\n",
    "- Another challenge is that in our design of this node class, we have to efficiently store and traverse the full computational graph! Even though there are fewer mathematical operations, there is more memory necessary for reverse mode than for foward mode, and we need to make this traversal efficient.\n",
    "\n",
    "\n",
    "#### Other Extensions\n",
    "We are planning to expand our project in a few further ways:\n",
    "1. Adding support for multiple functions of multiple inputs\n",
    "- Note that our Milestone 2 code only works for scalar functions of a single input\n",
    "- Our software will change because we need to be able to take partial derivatives. In our current application, the dual part of the dual number acts as the derivative at a given step, but it does not handle taking, say, the derivative of x + y only with respect to y.\n",
    "- This presents challenges because it poses the question of how we should adapt our current dual numbers class to handle partial derivatives.\n",
    "- Our software will use `numpy` to handle higher-dimensional functions.\n",
    "\n",
    "2. Writing a sample application using AutoDiffPy functionalities\n",
    "- The autodifferentiation capabilities can be used for Newton's method or implementing a simple neural network.\n",
    "- This brings challenges because it is fairly separate than our current work, so it will take a lot of research to efficiently create these sample applications.\n",
    "\n",
    "3. Making our software installable via PyPI\n",
    "- The current project code can be installed and set up per the \"Installation\" section, which involves a `requirements.txt` file. In the next iteration, it will be distributed via PyPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broader Impact\n",
    "Our software provides a tool for users to efficiently compute derivatives and jacobian values. As a public package lisenced under the MIT License, this tool may be used in a number of areas ranging from scientific research, personal projects, and industry applications. While our package provides similar functionality to pre-existing AutoDiff software, releasing our own version still contributes to the public domain by providing a unique implementation and framework that others may find particularly suitable to their use case. \n",
    "\n",
    "As the authors of this package, there are a few consequences important to note. Firstly, while our library has undergone unit and integration testing, we cannot guarantee full correctness in all cases, especially under the conditions of large-scale usage. As with most software, this package may be subject to further iteration & fixes. There may be significant consequences if this AutoDiff library is used without user-written tests; therefore we expect any users to take responsibility for ensuring that their applications meet their expectations and that integrating this AutoDiff package does not make them susceptible to incorrect or unexpected results.\n",
    "\n",
    "## Software Inclusivity\n",
    "The aim of this package is to increase access to Autodifferentiation capabilities, for students, researchers, programmers, and others who find it applicable. Additionally, our code is released on Github to improve visibility of the internal implementation of such libraries. We welcome open-source contributions or suggestions, which can be submitted by forking this repository and creating a pull-request (this pull request can contain a simple txt file for suggestions). Pull requests will be reviewed and approved by the original authors, who may reach out to you for collaboration. As an international team, we are happy to work with individuals of any background including non-native English speakers, and will provide language accomodation if needed. For those who are visually-impaired, an audio recording of this README can be accessed in the below Google Drive link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "Automatic differentiation is an extremely powerful tool in a number of scientific fields. In particular, our team is interested in its application to a machine learning context, which is a growing area of research. Much of machine learning research today concerns performance, as developing effective models often requires learning on large datasets. Differentiation is an important tool in this area. It is at the core of optimization algorithms such as gradient descent and stochastic gradient descent that enable such models to train and improve their accuracy. Since most models take in high-input datasets, reverse mode automatic diffferentiation provides an effective way to compute derivatives in only two-passes. In addition, as backpropagation introduces highly complex functions, our package relieves a large degree of computational overhead associated with calculating symbolic derivatives in this case. This motivated our extension of reverse mode in our own AutoDiffPy library. In the future, this area will remain our focus area as we continue to optimize this extension and develop an iterative implementation (as a complement to the current recursive method). We also plan to use our package to develop a simple neural network and release this under the `applications/` folder. This project will act as a guide to users of our library, encouraging further ML and AI development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Our software will be licensed under the MIT license, which is a highly permissive license and places few restrictions on reuse. As our software project uses standard methods of AD, we do not plan to pursue a patent. Since there are different auto differentiation packages available (such as in NumPy, PyTorch, TensorFlow, MatLab/Fortran (with packages called openAD, ADiMat), C++ (ADLOC), ADiMat, etc.), and we want other software engineers to use ours, we did not want to pick a license that would deter users. We did not add an explicit patent grant because we are not doing anything that has not been done before, so patents are not a consideration for us. Also note that since `numpy` and `pytest` have BSD licenses, we don't need to copy over their licenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
