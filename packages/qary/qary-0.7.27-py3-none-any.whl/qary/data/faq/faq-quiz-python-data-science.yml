
-
    Q: What are the advantages and disadvantages of using undersampling to deal with class imbalance.
    A: Undersampling the majority class reduces the amount of training data which can cause overfitting to the remaining data and reduce the accuracy of your model. But the model will train faster and thus give you more opportunities to train more models and tune the hyperparameters better.
-
    Q: What are the advantages and disadvantages of using oversampling to deal with class imbalance.
    A: Oversampling the minority class increases the number examples in your training set by duplicating the minority class examples. This can improve model accuracy but increases the training time required. This can reduce the number of models you have time to train and reduce the hyperparameter tuning you can do to improve the model.
-
    Q: "What is the best way to deal with class imbalance. Hint: think of better approaches than undersampling the majority class or oversampling the minority class."
    A: Use the `class_weight='balanced'` argument in many Scikit-Learn classifiers. Or manually set the sample_weights vector to weight each sample in inverse proportion to its class's prevalence in the data. An even better approach is to optimally undersample the majority class using unsupervised learning "clustering" to label each training example. You can have as many unsupervised class labels on the features as you have samples in your minority class. This will insure you have the greatest diversity of examples to train your model on. This will be faster and more accurate than any of the other approaches.
